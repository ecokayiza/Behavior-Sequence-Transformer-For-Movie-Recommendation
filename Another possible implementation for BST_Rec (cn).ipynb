{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "916d8da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "users = pd.read_csv(\n",
    "    \"ml-1m/users.dat\",\n",
    "    sep=\"::\",\n",
    "    names=[\"user_id\", \"sex\", \"age_group\", \"occupation\", \"zip_code\"],\n",
    "    engine='python'\n",
    ")\n",
    "\n",
    "movies = pd.read_csv(\n",
    "    \"ml-1m/movies.dat\", sep=\"::\", names=[\"movie_id\", \"title\", \"genres\"],\n",
    "    engine='python',encoding='ISO-8859-1'\n",
    ")\n",
    "\n",
    "ratings = pd.read_csv(\n",
    "    \"ml-1m/ratings.dat\",\n",
    "    sep=\"::\",\n",
    "    names=[\"user_id\", \"movie_id\", \"rating\", \"unix_timestamp\"],\n",
    "    engine='python'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57e07ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users data:\n",
      "   user_id sex  age_group  occupation zip_code\n",
      "0        1   F          1          10    48067\n",
      "1        2   M         56          16    70072\n",
      "2        3   M         25          15    55117\n",
      "3        4   M         45           7    02460\n",
      "4        5   M         25          20    55455\n",
      "\n",
      "Movies data:\n",
      "   movie_id                               title                        genres\n",
      "0         1                    Toy Story (1995)   Animation|Children's|Comedy\n",
      "1         2                      Jumanji (1995)  Adventure|Children's|Fantasy\n",
      "2         3             Grumpier Old Men (1995)                Comedy|Romance\n",
      "3         4            Waiting to Exhale (1995)                  Comedy|Drama\n",
      "4         5  Father of the Bride Part II (1995)                        Comedy\n",
      "\n",
      "Ratings data:\n",
      "   user_id  movie_id  rating  unix_timestamp\n",
      "0        1      1193       5       978300760\n",
      "1        1       661       3       978302109\n",
      "2        1       914       3       978301968\n",
      "3        1      3408       4       978300275\n",
      "4        1      2355       5       978824291\n"
     ]
    }
   ],
   "source": [
    "print(\"Users data:\")\n",
    "print(users.head())\n",
    "print(\"\\nMovies data:\")\n",
    "print(movies.head())\n",
    "print(\"\\nRatings data:\")\n",
    "print(ratings.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2ef0880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将原始ID转换为连续的索引，便于embedding层使用\n",
    "def generate_remap_id_dict(df,col):\n",
    "    ids = df[df[col].notnull()][col].unique().tolist()\n",
    "    ids = sorted(ids)\n",
    "    id_map_dict = {x: i+1 for i, x in enumerate(ids)}\n",
    "    id_map_dict[\"UNK\"]=0\n",
    "    df[f\"{col}_index\"] = df[col].fillna(\"UNK\").map(id_map_dict)\n",
    "    return id_map_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bba3a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_map_dict=generate_remap_id_dict(users,col='user_id')\n",
    "user_sex_map_dict=generate_remap_id_dict(users,col='sex')\n",
    "user_age_group_map_dict=generate_remap_id_dict(users,col='age_group')\n",
    "user_occupation_map_dict=generate_remap_id_dict(users,col='occupation')\n",
    "movie_id_map_dict = generate_remap_id_dict(movies,col='movie_id')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092d6773",
   "metadata": {},
   "source": [
    "评分缩放 1-5 => 0-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f21065f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "min_max_scaler = MinMaxScaler()\n",
    "ratings[\"norm_rating\"] = min_max_scaler.fit_transform(\n",
    "    ratings[\"rating\"].values.reshape(-1, 1))[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76972a57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['min_max_scaler.save']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save scaler\n",
    "import joblib\n",
    "joblib.dump(min_max_scaler, 'min_max_scaler.save')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09e1ff9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>sex</th>\n",
       "      <th>age_group</th>\n",
       "      <th>occupation</th>\n",
       "      <th>zip_code</th>\n",
       "      <th>user_id_index</th>\n",
       "      <th>sex_index</th>\n",
       "      <th>age_group_index</th>\n",
       "      <th>occupation_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>56</td>\n",
       "      <td>16</td>\n",
       "      <td>70072</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>15</td>\n",
       "      <td>55117</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>M</td>\n",
       "      <td>45</td>\n",
       "      <td>7</td>\n",
       "      <td>02460</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>20</td>\n",
       "      <td>55455</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id sex  age_group  occupation zip_code  user_id_index  sex_index  \\\n",
       "0        1   F          1          10    48067              1          1   \n",
       "1        2   M         56          16    70072              2          2   \n",
       "2        3   M         25          15    55117              3          2   \n",
       "3        4   M         45           7    02460              4          2   \n",
       "4        5   M         25          20    55455              5          2   \n",
       "\n",
       "   age_group_index  occupation_index  \n",
       "0                1                11  \n",
       "1                7                17  \n",
       "2                3                16  \n",
       "3                5                 8  \n",
       "4                3                21  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3961b083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id_index</th>\n",
       "      <th>sex_index</th>\n",
       "      <th>age_group_index</th>\n",
       "      <th>occupation_index</th>\n",
       "      <th>movie_id_index</th>\n",
       "      <th>norm_rating</th>\n",
       "      <th>unix_timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1177</td>\n",
       "      <td>1.00</td>\n",
       "      <td>978300760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>656</td>\n",
       "      <td>0.50</td>\n",
       "      <td>978302109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>903</td>\n",
       "      <td>0.50</td>\n",
       "      <td>978301968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>3340</td>\n",
       "      <td>0.75</td>\n",
       "      <td>978300275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>2287</td>\n",
       "      <td>1.00</td>\n",
       "      <td>978824291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id_index  sex_index  age_group_index  occupation_index  \\\n",
       "0              1          1                1                11   \n",
       "1              1          1                1                11   \n",
       "2              1          1                1                11   \n",
       "3              1          1                1                11   \n",
       "4              1          1                1                11   \n",
       "\n",
       "   movie_id_index  norm_rating  unix_timestamp  \n",
       "0            1177         1.00       978300760  \n",
       "1             656         0.50       978302109  \n",
       "2             903         0.50       978301968  \n",
       "3            3340         0.75       978300275  \n",
       "4            2287         1.00       978824291  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_user_full_matrix = users.merge(ratings[['user_id', 'movie_id', 'norm_rating','unix_timestamp']], on='user_id', how='left')\n",
    "df_user_full_matrix['movie_id_index'] = ratings['movie_id'].map(movie_id_map_dict)\n",
    "df_user_full_matrix['user_id_index'] = ratings['user_id'].map(user_id_map_dict)\n",
    "df_user_full_matrix['sex_index'] = df_user_full_matrix['sex'].map(user_sex_map_dict)\n",
    "df_user_full_matrix['age_group_index'] = df_user_full_matrix['age_group'].map(user_age_group_map_dict)\n",
    "df_user_full_matrix['occupation_index'] = df_user_full_matrix['occupation'].map(user_occupation_map_dict)\n",
    "df_user_full_matrix = df_user_full_matrix[['user_id_index', 'sex_index', 'age_group_index', 'occupation_index', 'movie_id_index', 'norm_rating','unix_timestamp']]\n",
    "df_user_full_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4508b6db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id_index</th>\n",
       "      <th>sex_index</th>\n",
       "      <th>age_group_index</th>\n",
       "      <th>occupation_index</th>\n",
       "      <th>movie_id_index</th>\n",
       "      <th>norm_rating</th>\n",
       "      <th>unix_timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>3118</td>\n",
       "      <td>0.75</td>\n",
       "      <td>978300019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1251</td>\n",
       "      <td>1.00</td>\n",
       "      <td>978300055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1673</td>\n",
       "      <td>0.75</td>\n",
       "      <td>978300055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1010</td>\n",
       "      <td>1.00</td>\n",
       "      <td>978300055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>2272</td>\n",
       "      <td>0.50</td>\n",
       "      <td>978300103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    user_id_index  sex_index  age_group_index  occupation_index  \\\n",
       "31              1          1                1                11   \n",
       "22              1          1                1                11   \n",
       "27              1          1                1                11   \n",
       "37              1          1                1                11   \n",
       "24              1          1                1                11   \n",
       "\n",
       "    movie_id_index  norm_rating  unix_timestamp  \n",
       "31            3118         0.75       978300019  \n",
       "22            1251         1.00       978300055  \n",
       "27            1673         0.75       978300055  \n",
       "37            1010         1.00       978300055  \n",
       "24            2272         0.50       978300103  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort with user_id and unix_timestamp\n",
    "df_user_full_matrix = df_user_full_matrix.sort_values(['user_id_index', 'unix_timestamp'])\n",
    "df_user_full_matrix.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa561dbe",
   "metadata": {},
   "source": [
    "\n",
    "### 输入特征\n",
    "```\n",
    "{\n",
    "    'user_id_index': 用户ID索引,\n",
    "    'movie_sequence': [电影1, 电影2, 电影3, 目标电影],  # 长度为4的序列\n",
    "    'rating_sequence': [评分1, 评分2, 评分3, 1.0],      # 前3个真实评分+掩码值\n",
    "    'target_movie': 目标电影ID,                          # 序列最后一个电影\n",
    "    'target_rating': 真实评分,                           # 预测目标\n",
    "    'sex_index': 性别特征,\n",
    "    'age_group_index': 年龄组索引,\n",
    "    'occupation_index': 职业索引\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77bda42e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id_index</th>\n",
       "      <th>movie_sequence</th>\n",
       "      <th>rating_sequence</th>\n",
       "      <th>target_movie</th>\n",
       "      <th>target_rating</th>\n",
       "      <th>sex_index</th>\n",
       "      <th>age_group_index</th>\n",
       "      <th>occupation_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[3118, 1251, 1673, 1010]</td>\n",
       "      <td>[0.75, 1.0, 0.75, 1.0]</td>\n",
       "      <td>1010</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[1673, 1010, 2272, 1769]</td>\n",
       "      <td>[0.75, 1.0, 0.5, 1.0]</td>\n",
       "      <td>1769</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>[2272, 1769, 3340, 2736]</td>\n",
       "      <td>[0.5, 1.0, 0.75, 1.0]</td>\n",
       "      <td>2736</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>[3340, 2736, 1190, 1177]</td>\n",
       "      <td>[0.75, 1.0, 0.75, 1.0]</td>\n",
       "      <td>1177</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>[1190, 1177, 712, 258]</td>\n",
       "      <td>[0.75, 1.0, 0.5, 1.0]</td>\n",
       "      <td>258</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id_index            movie_sequence         rating_sequence  \\\n",
       "0              1  [3118, 1251, 1673, 1010]  [0.75, 1.0, 0.75, 1.0]   \n",
       "1              1  [1673, 1010, 2272, 1769]   [0.75, 1.0, 0.5, 1.0]   \n",
       "2              1  [2272, 1769, 3340, 2736]   [0.5, 1.0, 0.75, 1.0]   \n",
       "3              1  [3340, 2736, 1190, 1177]  [0.75, 1.0, 0.75, 1.0]   \n",
       "4              1    [1190, 1177, 712, 258]   [0.75, 1.0, 0.5, 1.0]   \n",
       "\n",
       "   target_movie  target_rating  sex_index  age_group_index  occupation_index  \n",
       "0          1010           1.00          1                1                11  \n",
       "1          1769           1.00          1                1                11  \n",
       "2          2736           1.00          1                1                11  \n",
       "3          1177           1.00          1                1                11  \n",
       "4           258           0.75          1                1                11  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gen_sequence_data(df, window_size, step):\n",
    "    sequences = []\n",
    "    \n",
    "    for user_id, user_data in df.groupby('user_id_index'):\n",
    "        user_data = user_data.reset_index(drop=True)\n",
    "    \n",
    "        # 需要至少window_size个数据点才能创建一个序列\n",
    "        for i in range(0, len(user_data) - window_size, step):\n",
    "            sequence = user_data.iloc[i:i + window_size]\n",
    "            movie_sequence = sequence['movie_id_index'].tolist()  \n",
    "            rating_sequence = sequence['norm_rating'].tolist()    \n",
    "            target_movie = sequence['movie_id_index'].iloc[-1]         # 最后一个作为目标\n",
    "            target_rating = sequence['norm_rating'].iloc[-1]           # 最后一个作为目标\n",
    "            \n",
    "            # 对序列中的最后一个评分进行掩码处理（防止信息泄露）\n",
    "            masked_rating_sequence = rating_sequence[:-1] + [1.0]  \n",
    "            \n",
    "            sequences.append({\n",
    "                'user_id_index': user_id,\n",
    "                'movie_sequence': movie_sequence,\n",
    "                'rating_sequence': masked_rating_sequence,\n",
    "                'target_movie': target_movie,\n",
    "                'target_rating': target_rating,\n",
    "                'sex_index': sequence['sex_index'].iloc[0],\n",
    "                'age_group_index': sequence['age_group_index'].iloc[0],\n",
    "                'occupation_index': sequence['occupation_index'].iloc[0]\n",
    "            })\n",
    "    return pd.DataFrame(sequences)\n",
    "\n",
    "df_user_view = gen_sequence_data(df_user_full_matrix,window_size=4,step=2)\n",
    "df_user_view.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e4222d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "415550 73956\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "random_selection = np.random.rand(len(df_user_view)) <= 0.85\n",
    "train_data = df_user_view[random_selection]\n",
    "test_data = df_user_view[~random_selection]\n",
    "print(len(train_data),len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56f4be94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Dict, List, Optional, Union\n",
    "import math\n",
    "\n",
    "class SequenceEmbedding(nn.Module):\n",
    "    def __init__(self, num_item, embed_dim,seq_length):\n",
    "        super().__init__()\n",
    "        self.item_embedding = nn.Embedding(num_item, embed_dim, padding_idx=0)\n",
    "        self.position_embedding = nn.Embedding(seq_length, embed_dim)\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(embed_dim)  # add\n",
    "        \n",
    "    def forward(self, movie_sequence, rating_sequence=None):\n",
    "        batch_size, seq_len = movie_sequence.shape\n",
    "        \n",
    "        item_embeds = self.item_embedding(movie_sequence)  # (B, L, D)\n",
    "        \n",
    "        # Position embeddings\n",
    "        positions = torch.arange(seq_len, device=movie_sequence.device)\n",
    "        pos_embeds = self.position_embedding(positions).unsqueeze(0).expand(batch_size, -1, -1)\n",
    "\n",
    "        sequence_embeds = item_embeds + pos_embeds  # (B, L, D)\n",
    "\n",
    "        # Rating weighting (BST uses ratings as attention weights)\n",
    "        if rating_sequence is not None:\n",
    "            rating_weights = rating_sequence.unsqueeze(-1)  # (B, L, 1)\n",
    "            sequence_embeds = sequence_embeds * rating_weights\n",
    "            \n",
    "        return self.layer_norm(sequence_embeds)\n",
    "\n",
    "class EmbeddingLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    BST模型的统一Embedding层\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 embed_configs: Dict[str, Dict],\n",
    "                 dropout: float = 0.2,\n",
    "                 initialization: str = \"xavier\"):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embed_configs = embed_configs\n",
    "        self.dropout = dropout\n",
    "        self.embed_dim = embed_configs['position']['embed_dim']\n",
    "        self.seq_len = embed_configs['position']['num_embed']\n",
    "        \n",
    "        \n",
    "        # 创建embedding层\n",
    "        self.embeddings = nn.ModuleDict() # 各特征的Embedding\n",
    "        self.feature_types = {}\n",
    "        \n",
    "        for feature_name, config in embed_configs.items():\n",
    "            embed_dim_feat = config.get('embed_dim',self.embed_dim)\n",
    "            num_embeddings = config['num_embed']\n",
    "            feature_type = config.get('type', 'categorical')\n",
    "            \n",
    "            # 根据特征类型创建不同的embedding\n",
    "            if feature_type == 'categorical':\n",
    "                self.embeddings[feature_name] = nn.Embedding(\n",
    "                    num_embeddings, embed_dim_feat, padding_idx=0\n",
    "                )\n",
    "            elif feature_type == 'sequence':\n",
    "                # 序列特征\n",
    "                seq_length = config.get('seq_length', self.seq_len)\n",
    "                self.embeddings[feature_name] = SequenceEmbedding(\n",
    "                    num_embeddings, embed_dim_feat, seq_length\n",
    "                )\n",
    "            \n",
    "            self.feature_types[feature_name] = feature_type\n",
    "            \n",
    "            # 初始化embedding权重\n",
    "            self._init_embedding(self.embeddings[feature_name], initialization)\n",
    "            # inner\n",
    "        # outer\n",
    "        # Dropout层\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Transformer输入维度\n",
    "        self.transformer_dim = embed_configs['item']['embed_dim']\n",
    "        \n",
    "        # MLP输入维度：原始特征 + 交叉特征 + 目标电影 + 序列特征\n",
    "        user_dim = embed_configs['user']['embed_dim']  # 32\n",
    "        occupation_dim = embed_configs['occupation']['embed_dim']  # 32\n",
    "        age_group_dim = embed_configs['age_group']['embed_dim']  # 32\n",
    "        sex_dim = embed_configs['sex']['embed_dim']  # 32\n",
    "        target_movie_dim = embed_configs['item']['embed_dim']  # 32\n",
    "        sequence_dim = self.transformer_dim * self.seq_len  # 64 * 4 = 256\n",
    "        \n",
    "        self.total_dim = (user_dim + occupation_dim + age_group_dim + sex_dim +  # 原始特征: 128\n",
    "                         user_dim + occupation_dim + age_group_dim + sex_dim +   # 交叉特征: 128  \n",
    "                         target_movie_dim +  # 目标电影: 32\n",
    "                         sequence_dim)  # 序列特征: 256\n",
    "        # 总计: 128 + 128 + 32 + 256 = 544\n",
    "        \n",
    "        \n",
    "    def _init_embedding(self, embedding_layer, init_type):\n",
    "        \"\"\"初始化embedding权重\"\"\"\n",
    "        if hasattr(embedding_layer, 'weight'):\n",
    "            if init_type == \"xavier\":\n",
    "                nn.init.xavier_uniform_(embedding_layer.weight)\n",
    "            elif init_type == \"normal\":\n",
    "                nn.init.normal_(embedding_layer.weight, std=0.1)\n",
    "            elif init_type == \"kaiming\":\n",
    "                nn.init.kaiming_uniform_(embedding_layer.weight)\n",
    "    \n",
    "    def forward(self, features: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        embeddings = {}\n",
    "        \n",
    "        for feature_name, feature_tensor in features.items():\n",
    "            if feature_name in self.embeddings:\n",
    "                # 获取embedding\n",
    "                embed = self.embeddings[feature_name](feature_tensor)\n",
    "                # 应用dropout\n",
    "                embed = self.embedding_dropout(embed)\n",
    "                embeddings[feature_name] = embed\n",
    "        return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b9674b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding config\n",
    "\n",
    "num_user = len(user_id_map_dict)\n",
    "num_movie = len(movie_id_map_dict)\n",
    "num_occupation = len(user_occupation_map_dict)\n",
    "num_age_group = len(user_age_group_map_dict)\n",
    "num_sex = len(user_sex_map_dict)\n",
    "\n",
    "embed_configs = {}\n",
    "EMED_DIM=32\n",
    "SEQUENCE_SIZE = 4\n",
    "\n",
    "embed_configs['item']={\"embed_dim\":EMED_DIM,\"num_embed\":num_movie}\n",
    "embed_configs['position'] = {\"embed_dim\":EMED_DIM,\"num_embed\":SEQUENCE_SIZE}\n",
    "# sequence类型\n",
    "embed_configs['sequence'] = {\n",
    "    \"embed_dim\": EMED_DIM, \n",
    "    \"num_embed\": num_movie, \n",
    "    \"type\": \"sequence\",  \n",
    "    \"seq_length\": SEQUENCE_SIZE\n",
    "}\n",
    "\n",
    "embed_configs['user']={\"embed_dim\":EMED_DIM,\"num_embed\":num_user}\n",
    "embed_configs['sex'] = {\"embed_dim\": EMED_DIM, \"num_embed\":num_sex }\n",
    "embed_configs['occupation']={\"embed_dim\":EMED_DIM,\"num_embed\":num_occupation}\n",
    "embed_configs['age_group']={\"embed_dim\":EMED_DIM,\"num_embed\":num_age_group}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba10f9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModuleDict(\n",
      "  (item): Embedding(3884, 32, padding_idx=0)\n",
      "  (position): Embedding(4, 32, padding_idx=0)\n",
      "  (sequence): SequenceEmbedding(\n",
      "    (item_embedding): Embedding(3884, 32, padding_idx=0)\n",
      "    (position_embedding): Embedding(4, 32)\n",
      "    (layer_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (user): Embedding(6041, 32, padding_idx=0)\n",
      "  (sex): Embedding(3, 32, padding_idx=0)\n",
      "  (occupation): Embedding(22, 32, padding_idx=0)\n",
      "  (age_group): Embedding(8, 32, padding_idx=0)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = EmbeddingLayer(embed_configs)\n",
    "print(embedding_layer.embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e39095af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_heads, dropout_rate):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "\n",
    "        self.multihead_attention = nn.MultiheadAttention(input_size, num_heads)\n",
    "        self.layer_norm1 = nn.LayerNorm(input_size)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(input_size, 4*input_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*input_size, output_size),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        self.layer_norm2 = nn.LayerNorm(output_size)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Multi-head Attention\n",
    "        attn_output, _ = self.multihead_attention(x, x, x)\n",
    "        x = self.layer_norm1(x + attn_output)\n",
    "\n",
    "        # Feed-Forward Network\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.layer_norm2(x + ff_output)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads=8, dropout_rate=0.2, num_layers=3):\n",
    "        super(TransformerLayer, self).__init__()\n",
    "\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, d_model, num_heads, dropout_rate)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for transformer_block in self.transformer_blocks:\n",
    "            x = transformer_block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3f1ceecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, dropout=0.2, hidden_units=[512, 256,128]):\n",
    "        super(MLP, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(len(hidden_units) - 1):\n",
    "            self.layers.append(nn.Linear(hidden_units[i], hidden_units[i + 1]))\n",
    "            self.layers.append(nn.LeakyReLU())\n",
    "            self.layers.append(nn.Dropout(p=dropout))\n",
    "        self.fc = nn.Linear(hidden_units[-1],1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        logits = self.fc(x)\n",
    "        output = self.sigmoid(logits)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2dfb4bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BSTRecommender(nn.Module):    \n",
    "    def __init__(self,embedding_layer,num_heads=8,transformer_num_layer=3,drop_out=0.2):\n",
    "        super().__init__()\n",
    "        # Embedding params\n",
    "        self.seq_len = embedding_layer.seq_len\n",
    "        self.totoal_dim = embedding_layer.total_dim\n",
    "        self.transformer_dim = embedding_layer.transformer_dim\n",
    "        \n",
    "        self.drouput = drop_out\n",
    "        self.num_heads = num_heads\n",
    "        self.transformer_num_layer = transformer_num_layer\n",
    "        \n",
    "        # Embedding\n",
    "        self.embedding_layer = embedding_layer\n",
    "        \n",
    "        # Transformer\n",
    "        self.transformer_layer = TransformerLayer(d_model=self.transformer_dim,\n",
    "                                            num_heads=self.num_heads,\n",
    "                                            dropout_rate=self.drouput,\n",
    "                                            num_layers=self.transformer_num_layer)\n",
    "        \n",
    "        # MLP\n",
    "        self.mlp = MLP(dropout=self.drouput, hidden_units=[self.totoal_dim, 256, 64])\n",
    "            \n",
    "    def forward(self, batch):\n",
    "        batch_size = batch['movie_sequence'].shape[0]\n",
    "        \n",
    "        # 用户特征\n",
    "        user_embed = self.embedding_layer.embeddings['user'](batch['user_id_index'])\n",
    "        occupation_embed = self.embedding_layer.embeddings['occupation'](batch['occupation_index'])\n",
    "        age_group_embed = self.embedding_layer.embeddings['age_group'](batch['age_group_index'])\n",
    "        sex_embed = self.embedding_layer.embeddings['sex'](batch['sex'])\n",
    "        target_movie_embed = self.embedding_layer.embeddings['item'](batch['target_movie'])\n",
    "        # 序列特征\n",
    "        sequence_embeds = self.embedding_layer.embeddings['sequence'](\n",
    "            batch['movie_sequence'], \n",
    "            batch.get('rating_sequence', None)\n",
    "        )\n",
    "        \n",
    "        # Transformer编码\n",
    "        transformer_output = self.transformer_layer(sequence_embeds)\n",
    "        sequence_pooled = transformer_output.view(batch_size, -1) # Flatten\n",
    "        \n",
    "        # 特征交叉\n",
    "        user_cross = torch.mul(user_embed, target_movie_embed)\n",
    "        occupation_cross = torch.mul(occupation_embed, target_movie_embed)\n",
    "        age_group_cross = torch.mul(age_group_embed, target_movie_embed)\n",
    "        sex_cross = torch.mul(sex_embed, target_movie_embed)\n",
    "        # 特征融合\n",
    "        features = torch.cat([\n",
    "        user_embed, occupation_embed, age_group_embed, sex_embed, # 原始特征\n",
    "        user_cross, occupation_cross, age_group_cross, sex_cross, # 交叉特征\n",
    "        target_movie_embed, # 目标电影特征\n",
    "        sequence_pooled # 序列特征\n",
    "        ], dim=-1)\n",
    "        \n",
    "        # MLP预测\n",
    "        output = self.mlp(features)  # Use self.mlp, not self.mlp_predictor\n",
    "        \n",
    "        return output.squeeze(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "52bfae93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BSTRecommender(\n",
       "  (embedding_layer): EmbeddingLayer(\n",
       "    (embeddings): ModuleDict(\n",
       "      (item): Embedding(3884, 32, padding_idx=0)\n",
       "      (position): Embedding(4, 32, padding_idx=0)\n",
       "      (sequence): SequenceEmbedding(\n",
       "        (item_embedding): Embedding(3884, 32, padding_idx=0)\n",
       "        (position_embedding): Embedding(4, 32)\n",
       "        (layer_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (user): Embedding(6041, 32, padding_idx=0)\n",
       "      (sex): Embedding(3, 32, padding_idx=0)\n",
       "      (occupation): Embedding(22, 32, padding_idx=0)\n",
       "      (age_group): Embedding(8, 32, padding_idx=0)\n",
       "    )\n",
       "    (embedding_dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (transformer_layer): TransformerLayer(\n",
       "    (transformer_blocks): ModuleList(\n",
       "      (0-2): 3 x TransformerBlock(\n",
       "        (multihead_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): Sequential(\n",
       "          (0): Linear(in_features=32, out_features=128, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=128, out_features=32, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (mlp): MLP(\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0): Linear(in_features=416, out_features=256, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Dropout(p=0.2, inplace=False)\n",
       "      (3): Linear(in_features=256, out_features=64, bias=True)\n",
       "      (4): LeakyReLU(negative_slope=0.01)\n",
       "      (5): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "    (fc): Linear(in_features=64, out_features=1, bias=True)\n",
       "    (sigmoid): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(DEVICE)\n",
    "model = BSTRecommender(embedding_layer=embedding_layer)\n",
    "model.to(DEVICE)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9c9fa12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class BSTDataset(Dataset):\n",
    "    def __init__(self, data, device):\n",
    "        self.data = data\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        \n",
    "        return {\n",
    "            'user_id_index': torch.tensor(row['user_id_index'], dtype=torch.long).to(self.device),\n",
    "            'occupation_index': torch.tensor(row['occupation_index'], dtype=torch.long).to(self.device),\n",
    "            'age_group_index': torch.tensor(row['age_group_index'], dtype=torch.long).to(self.device),\n",
    "            'sex': torch.tensor(row['sex_index'], dtype=torch.long).to(self.device),\n",
    "            'movie_sequence': torch.tensor(row['movie_sequence'], dtype=torch.long).to(self.device),\n",
    "            'rating_sequence': torch.tensor(row['rating_sequence'], dtype=torch.float).to(self.device),\n",
    "            'target_movie': torch.tensor(row['target_movie'], dtype=torch.long).to(self.device),  \n",
    "            'target_rating': torch.tensor(row['target_rating'], dtype=torch.float).to(self.device)  \n",
    "        }\n",
    "# Create datasets\n",
    "train_dataset = BSTDataset(train_data.reset_index(drop=True), DEVICE)\n",
    "test_dataset = BSTDataset(test_data.reset_index(drop=True), DEVICE)\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4c1a3b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training config\n",
    "epochs = 20\n",
    "learning_rate = 0.001\n",
    "criterion = nn.L1Loss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a7626d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20:   0%|          | 0/1624 [00:00<?, ?it/s, loss=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 1624/1624 [04:42<00:00,  5.74it/s, loss=0.166]\n",
      "Testing: 100%|██████████| 289/289 [00:39<00:00,  7.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Loss: 0.1590 Test loss: 0.1698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 1624/1624 [04:00<00:00,  6.75it/s, loss=0.138]\n",
      "Testing: 100%|██████████| 289/289 [00:40<00:00,  7.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Average Loss: 0.1535 Test loss: 0.1698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 1624/1624 [04:00<00:00,  6.76it/s, loss=0.134]\n",
      "Testing: 100%|██████████| 289/289 [00:40<00:00,  7.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Average Loss: 0.1511 Test loss: 0.1693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████| 1624/1624 [03:59<00:00,  6.77it/s, loss=0.174]\n",
      "Testing: 100%|██████████| 289/289 [00:40<00:00,  7.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Average Loss: 0.1488 Test loss: 0.1702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████| 1624/1624 [03:58<00:00,  6.80it/s, loss=0.131]\n",
      "Testing: 100%|██████████| 289/289 [00:39<00:00,  7.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Average Loss: 0.1467 Test loss: 0.1700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████| 1624/1624 [03:58<00:00,  6.81it/s, loss=0.171]\n",
      "Testing: 100%|██████████| 289/289 [00:40<00:00,  7.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Average Loss: 0.1447 Test loss: 0.1701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████| 1624/1624 [03:57<00:00,  6.84it/s, loss=0.151]\n",
      "Testing: 100%|██████████| 289/289 [00:40<00:00,  7.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Average Loss: 0.1429 Test loss: 0.1706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████| 1624/1624 [03:58<00:00,  6.81it/s, loss=0.167]\n",
      "Testing: 100%|██████████| 289/289 [00:40<00:00,  7.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Average Loss: 0.1415 Test loss: 0.1704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|██████████| 1624/1624 [03:58<00:00,  6.80it/s, loss=0.153]\n",
      "Testing: 100%|██████████| 289/289 [00:40<00:00,  7.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Average Loss: 0.1398 Test loss: 0.1709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|██████████| 1624/1624 [03:58<00:00,  6.82it/s, loss=0.129]\n",
      "Testing: 100%|██████████| 289/289 [00:40<00:00,  7.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Average Loss: 0.1385 Test loss: 0.1710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|██████████| 1624/1624 [03:57<00:00,  6.84it/s, loss=0.173]\n",
      "Testing: 100%|██████████| 289/289 [00:40<00:00,  7.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Average Loss: 0.1370 Test loss: 0.1709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|██████████| 1624/1624 [03:58<00:00,  6.82it/s, loss=0.13] \n",
      "Testing: 100%|██████████| 289/289 [00:40<00:00,  7.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Average Loss: 0.1360 Test loss: 0.1708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|██████████| 1624/1624 [03:58<00:00,  6.80it/s, loss=0.196]\n",
      "Testing: 100%|██████████| 289/289 [00:40<00:00,  7.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Average Loss: 0.1348 Test loss: 0.1704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|██████████| 1624/1624 [03:57<00:00,  6.84it/s, loss=0.136]\n",
      "Testing: 100%|██████████| 289/289 [00:40<00:00,  7.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Average Loss: 0.1336 Test loss: 0.1703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|██████████| 1624/1624 [03:58<00:00,  6.80it/s, loss=0.121] \n",
      "Testing: 100%|██████████| 289/289 [00:40<00:00,  7.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Average Loss: 0.1325 Test loss: 0.1707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|██████████| 1624/1624 [03:58<00:00,  6.81it/s, loss=0.101]\n",
      "Testing: 100%|██████████| 289/289 [00:40<00:00,  7.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Average Loss: 0.1315 Test loss: 0.1715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|██████████| 1624/1624 [03:59<00:00,  6.79it/s, loss=0.141]\n",
      "Testing: 100%|██████████| 289/289 [00:40<00:00,  7.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Average Loss: 0.1304 Test loss: 0.1711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|██████████| 1624/1624 [03:56<00:00,  6.86it/s, loss=0.11]  \n",
      "Testing: 100%|██████████| 289/289 [00:40<00:00,  7.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Average Loss: 0.1295 Test loss: 0.1716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|██████████| 1624/1624 [03:57<00:00,  6.84it/s, loss=0.136] \n",
      "Testing: 100%|██████████| 289/289 [00:39<00:00,  7.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Average Loss: 0.1286 Test loss: 0.1713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|██████████| 1624/1624 [03:58<00:00,  6.80it/s, loss=0.115]\n",
      "Testing: 100%|██████████| 289/289 [00:40<00:00,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Average Loss: 0.1279 Test loss: 0.1712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "test_scores = []\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}', postfix={'loss': 0} )\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        predictions = model(batch)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(predictions, batch['target_rating'])\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix({'loss': loss.item()})\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    avg_test_loss, predictions_list, targets_list = test()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}, Average Loss: {avg_loss:.4f} Test loss: {avg_test_loss:.4f}')\n",
    "    # Save checkpoint after each epoch\n",
    "    test_scores.append(avg_test_loss)\n",
    "    torch.save(model.state_dict(), f'./checkpoints/bst_model_epoch_{epoch+1}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "52d6b6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as bst_model.pth\n"
     ]
    }
   ],
   "source": [
    "# Save modelw\n",
    "torch.save(model.state_dict(), 'bst_model.pth')\n",
    "print('Model saved as bst_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d254faae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.16980269030509937, 0.16975156328669883, 0.1693197827541292, 0.17021437571947962, 0.17001508980680088, 0.17006768935898184, 0.1705918589471533, 0.17039927731954516, 0.17088893641134448, 0.17095972705877363, 0.1708614778250559, 0.1707887686144522, 0.17040371637030868, 0.17032126247057866, 0.1706961907936215, 0.17151299144776222, 0.17114394310230202, 0.1716276722280212, 0.17132704248684089, 0.1711991835630476]\n"
     ]
    }
   ],
   "source": [
    "print(test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3a087c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./checkpoints/bst_model_epoch_3.pth\n",
      "BST Model loaded from bst_model.pth\n"
     ]
    }
   ],
   "source": [
    "def load_bst_model(model_path='bst_model.pth', device=DEVICE):\n",
    "    embed_configs = {\n",
    "        'item': {\"embed_dim\": EMED_DIM, \"num_embed\": len(movie_id_map_dict)},\n",
    "        'position': {\"embed_dim\": EMED_DIM, \"num_embed\": SEQUENCE_SIZE},\n",
    "        'user': {\"embed_dim\": EMED_DIM, \"num_embed\": len(user_id_map_dict)},\n",
    "        'sex': {\"embed_dim\": EMED_DIM, \"num_embed\": len(user_sex_map_dict)},\n",
    "        'occupation': {\"embed_dim\": EMED_DIM, \"num_embed\": len(user_occupation_map_dict)},\n",
    "        'age_group': {\"embed_dim\": EMED_DIM, \"num_embed\": len(user_age_group_map_dict)},\n",
    "        'sequence': {\n",
    "            \"embed_dim\": EMED_DIM,\n",
    "            \"num_embed\": num_movie,\n",
    "            \"type\": \"sequence\",\n",
    "            \"seq_length\": SEQUENCE_SIZE\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    embedding_layer = EmbeddingLayer(embed_configs)\n",
    "    model = BSTRecommender(embedding_layer=embedding_layer)\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "# load BST model\n",
    "best_path = f'./checkpoints/bst_model_epoch_{np.argmin(test_scores)+1}.pth'\n",
    "print(best_path)\n",
    "bst_model = load_bst_model(model_path=best_path, device=DEVICE)\n",
    "print('BST Model loaded from bst_model.pth')\n",
    "min_max_scaler = joblib.load('min_max_scaler.save')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "94394580",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 289/289 [00:42<00:00,  6.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1712\n",
      "MAE: 0.1712 (0.6848)\n",
      "RMSE: 0.2467 (0.9867)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# test\n",
    "def test():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_loss = 0\n",
    "        predictions_list = []\n",
    "        targets_list = []\n",
    "        \n",
    "        for batch in tqdm(test_loader, desc='Testing'):\n",
    "            predictions = model(batch)\n",
    "            loss = criterion(predictions, batch['target_rating'])\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            predictions_list.extend(predictions.cpu().numpy())\n",
    "            targets_list.extend(batch['target_rating'].cpu().numpy())\n",
    "\n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    return avg_test_loss, predictions_list, targets_list\n",
    "avg_test_loss, predictions_list, targets_list = test()\n",
    "print(f'Test Loss: {avg_test_loss:.4f}')\n",
    "\n",
    "# Calculate additional metrics\n",
    "\n",
    "mae = mean_absolute_error(targets_list, predictions_list)\n",
    "rmse = np.sqrt(mean_squared_error(targets_list, predictions_list))\n",
    "\n",
    "original_targets = min_max_scaler.inverse_transform(np.array(targets_list).reshape(-1,1))[:,0]\n",
    "original_predictions = min_max_scaler.inverse_transform(np.array(predictions_list).reshape(-1,1))[:,0]\n",
    "\n",
    "original_mae = mean_absolute_error(original_targets, original_predictions)\n",
    "original_rmse = np.sqrt(mean_squared_error(original_targets, original_predictions))\n",
    "\n",
    "print(f'MAE: {mae:.4f} ({original_mae:.4f})')\n",
    "print(f'RMSE: {rmse:.4f} ({original_rmse:.4f})')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
